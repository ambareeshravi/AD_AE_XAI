{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:559: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from model_utils import *\n",
    "from data import *\n",
    "from c2d_models import *\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "# https://github.com/marcoancona/DeepExplain\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "from lime import lime_image\n",
    "\n",
    "# https://pypi.org/project/keras-explain/\n",
    "from keras_explain.lrp import LRP\n",
    "from keras_explain.grad_cam import GradCam\n",
    "from keras_explain.guided_bp import GuidedBP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"trained_models/C2D_AE_128_3x3_HAM10000/model.h5\"\n",
    "\n",
    "def get_model(model_path, rec = True, max_value=1000):\n",
    "    if rec: model = C2D_AE_128_3x3(isTrain = True)\n",
    "    else: model = C2D_AE_128_3x3(isTrain = False, max_value = max_value)\n",
    "    model.model.load_weights(model_path)\n",
    "    return model.model\n",
    "\n",
    "# c2d_model_prb = get_model(False)\n",
    "# c2d_model_rec = get_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"HAM10000\"\n",
    "TEST_DIR = join_paths([\"TEST_SETS/\", DATASET, \"\"])\n",
    "\n",
    "test_dict = dict()\n",
    "for class_dir in read_directory_contents(TEST_DIR):\n",
    "    test_dict[class_dir.split(\"/\")[-1]] = np.array([read_image(file, resize_to=(128,128))/255. for file in read_directory_contents(class_dir)])\n",
    "    \n",
    "# 1 -> High Loss -> Anomaly\n",
    "# 0 -> Low Loss -> Normal\n",
    "X_test = list()\n",
    "y_test = list()\n",
    "for key in set(test_dict.keys()):\n",
    "    X_test.append(test_dict[key])\n",
    "    y_test += [0 if \"normal\" in key else 1]*len(test_dict[key])\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_methods = {\n",
    "    \"Gradient_Inputs\": \"grad*input\",\n",
    "    \"Saliency\": \"saliency\",\n",
    "    \"Integrated_Gradients\": \"intgrad\",\n",
    "    \"DeepLIFT\": \"deeplift\",\n",
    "    \"e-LRP\": \"elrp\",\n",
    "    \"Occlusion\": \"occlusion\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "----------------------------------------\n",
      "Method: Gradient_Inputs\n",
      "----------------------------------------\n",
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:638: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:638: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:76: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:76: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Method: Saliency\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Method: Integrated_Gradients\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Method: DeepLIFT\n",
      "----------------------------------------\n",
      "ERROR: 'NoneType' object cannot be interpreted as an integer\n",
      "----------------------------------------\n",
      "Method: e-LRP\n",
      "----------------------------------------\n",
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:336: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ambreesh/Documents/amb_venv/src/deepexplain/deepexplain/tensorflow/methods.py:336: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Method: Occlusion\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=K.get_session()) as de:\n",
    "    model = get_model(True)\n",
    "    input_tensor = model.layers[0].input\n",
    "    de_model = Model(inputs=input_tensor, outputs = model.layers[-1].output)\n",
    "    target_tensor = de_model(input_tensor)\n",
    "\n",
    "    for method_name, method_tag in de_methods.items():\n",
    "        print(\"-\"*40)\n",
    "        print(\"Method: %s\"%method_name)\n",
    "        print(\"-\"*40)\n",
    "        try: attributions = de.explain(method_tag, target_tensor, input_tensor, X_test, ys=X_test, batch_size = 64) # np.expand_dims(y_test, axis = -1))\n",
    "        except Exception as e: \n",
    "            print(\"ERROR:\", e)\n",
    "            continue\n",
    "        xai_results[method_name] = attributions\n",
    "\n",
    "#     print(\"-\"*40)\n",
    "#     print(\"Method: %s\"%method_name)\n",
    "#     print(\"-\"*40)\n",
    "#     try: xai_results[\"Shapely\"] = de.explain('shapley_sampling', target_tensor, input_tensor, X_test, ys=X_test, samples=2, sampling_dims=[1,2,3])\n",
    "#     except Exception as e: print(\"ERROR:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _lime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(False)\n",
    "le = LimeExplainer(model, num_features = 128*128, num_samples = 1000, batch_size = 64)\n",
    "lime_results = le.explain(X_test, results_path = \"\", mask_features = 5, anomaly_type = None, save_result = False)\n",
    "lime_results = np.array(lime_results)\n",
    "\n",
    "xai_results[\"LIME\"] = lime_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRP, GuidedBP, GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove /tmp/checkpoint and all other keras files in /tmp/ if GuidedBP isnt working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /tmp/checkpoint \n",
    "!rm /tmp/gb_keras.h5 \n",
    "!rm -rf /tmp/guided_backprop_ckpt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ke_methods = {\n",
    "    \"GuidedBP\": GuidedBP(model),\n",
    "    \"GradCAM\": GradCam(model, layer = -2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_explainer_results = dict([(key, list()) for key in list(ke_methods.keys())])\n",
    "\n",
    "for method, explainer in ke_methods.items():\n",
    "    for xs in X_test:\n",
    "        exp = explainer.explain(xs, 1)\n",
    "        k_explainer_results[method].append(normalize(exp[0]))\n",
    "        \n",
    "for key in k_explainer_results.keys():\n",
    "    xai_results[key] = np.array(k_explainer_results[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _shap import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_train = HAM10000()\n",
    "X_train = ir_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SHAP_Explainer(blend_alpha=0.45)\n",
    "shap_out = se.explain(model, X_train[:1000], X_test/255.)\n",
    "xai_results[\"SHAP\"] = shap_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xai_results.keys())\n",
    "dump_pickle(xai_results, \"results/xai_results_%s.pkl\"%(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LRP\n",
    "2. LIME\n",
    "3. SHAP\n",
    "4. COUNTER_FACTUALS\n",
    "5. GUIDED_BP\n",
    "6. GRAD_CAM\n",
    "7. GRADIENT_INPUTS\n",
    "8. INTEGRATED_GRADIENTS\n",
    "9. SALIENCY\n",
    "10. OCCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
